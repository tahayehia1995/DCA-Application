{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Noise Removing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script automates the detection and removal of outliers from CSV files in a specified directory.\n",
    "# It uses PyCaret for anomaly detection and a rolling window method to detect outliers.\n",
    "# The script supports user input to customize hyperparameters and select detection algorithms.\n",
    "# Results can be plotted or saved based on user preferences.\n",
    "\n",
    "import os  # For interacting with the file system\n",
    "import pandas as pd  # For data manipulation and analysis with DataFrames\n",
    "import matplotlib.pyplot as plt  # For creating visualizations\n",
    "from pycaret.anomaly import setup, create_model, assign_model  # For anomaly detection with PyCaret\n",
    "import re  # For regular expressions to find patterns in strings\n",
    "from collections import Counter  # For counting occurrences of elements in iterables\n",
    "\n",
    "# Initialize PyCaret for anomaly detection with the given dataset\n",
    "def setup_pycaret(data, normalize=True, transformation=False, transformation_method='quantile', numeric_imputation='mean'):\n",
    "    \"\"\"\n",
    "    Initializes the dataset for anomaly detection using PyCaret, with optional normalization and transformation.\n",
    "    \n",
    "    Args:\n",
    "    data (pd.DataFrame): The dataset to initialize for anomaly detection.\n",
    "    normalize (bool): Whether to normalize the data.\n",
    "    transformation (bool): Whether to apply data transformation.\n",
    "    transformation_method (str): The transformation method to use ('quantile' by default).\n",
    "    numeric_imputation (str): Method for imputing missing numerical values ('mean' by default).\n",
    "\n",
    "    Returns:\n",
    "    object: The PyCaret setup environment for anomaly detection.\n",
    "    \"\"\"\n",
    "    # Shuffle the data and reset the index to ensure randomness; use all data (frac=1) to detect outliers from\n",
    "    data = data.sample(frac=1, random_state=786)\n",
    "\n",
    "    # Initialize the PyCaret setup with the provided parameters\n",
    "    return setup(data,\n",
    "                 normalize=normalize,  # Normalize the data if specified\n",
    "                 transformation=transformation,  # Apply data transformation if specified\n",
    "                 numeric_imputation=numeric_imputation,  # Impute missing values numerically with mean or specified method\n",
    "                 transformation_method=transformation_method)  # Use specified transformation method\n",
    "\n",
    "# Function to detect outliers in a rolling window fashion\n",
    "def window_outliers(data, window_size=15, step_size=5, method='lowest_quantile', num_lowest_points=3):\n",
    "    \"\"\"\n",
    "    Detects outliers using a rolling window approach and returns indices of detected outliers.\n",
    "    \n",
    "    Args:\n",
    "    data (pd.Series): The data series to analyze for outliers.\n",
    "    window_size (int): The size of the rolling window (default is 15).\n",
    "    step_size (int): The step size for the rolling window (default is 5).\n",
    "    method (str): Method to use for outlier detection ('lowest_quantile', 'both_quantiles', or 'lowest_points').\n",
    "    num_lowest_points (int): Number of lowest points to consider as outliers if using 'lowest_points' method.\n",
    "\n",
    "    Returns:\n",
    "    list: List of indices corresponding to detected outliers.\n",
    "    \"\"\"\n",
    "    outliers = []  # List to store detected outlier indices\n",
    "    for i in range(0, len(data) - window_size + 1, step_size):\n",
    "        window = data.iloc[i:i + window_size]  # Get the data window of specified size\n",
    "        if method == 'lowest_quantile':\n",
    "            q1 = window.quantile(0.25)  # Calculate the first quartile\n",
    "            outlier_indices = window[window < q1].index  # Detect outliers below Q1\n",
    "        elif method == 'both_quantiles':\n",
    "            q1 = window.quantile(0.25)\n",
    "            q3 = window.quantile(0.75)  # Calculate the third quartile\n",
    "            outlier_indices = window[(window < q1) | (window > q3)].index  # Detect outliers below Q1 or above Q3\n",
    "        elif method == 'lowest_points':\n",
    "            outlier_indices = window.nsmallest(num_lowest_points).index  # Detect the smallest points as outliers\n",
    "        outliers.extend(outlier_indices)  # Add detected indices to the outlier list\n",
    "    return list(set(outliers))  # Return unique outlier indices\n",
    "\n",
    "# Function to count files with repeated IDs in a directory\n",
    "def count_repeated_files(directory):\n",
    "    \"\"\"\n",
    "    Counts the occurrence of files with repeated IDs in the specified directory.\n",
    "    \n",
    "    Args:\n",
    "    directory (str): The directory path to search for files.\n",
    "\n",
    "    Returns:\n",
    "    Counter: A Counter object with the counts of each unique ID.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'(?:gas_|oil_)([^_]+)_')  # Regex pattern to extract ID from filenames\n",
    "    counts = Counter()  # Counter to keep track of occurrences of each ID\n",
    "    for file in os.listdir(directory):  # Iterate over each file in the directory\n",
    "        match = pattern.search(file)  # Search for pattern in filenames\n",
    "        if match:\n",
    "            counts[match.group(1)] += 1  # Increment counter for the matched ID\n",
    "    for key, count in counts.items():\n",
    "        print(f\"ID: {key} - Count: {count}\")  # Print each ID and its count\n",
    "    return counts  # Return the counter\n",
    "\n",
    "# Function to get hyperparameters for anomaly detection algorithms from user input\n",
    "def get_hyperparameters(algorithm_choice, default_hyperparameters):\n",
    "    \"\"\"\n",
    "    Prompts user to input custom hyperparameters for the selected anomaly detection algorithm(s).\n",
    "    \n",
    "    Args:\n",
    "    algorithm_choice (str): The algorithm chosen by the user ('all' or a specific algorithm).\n",
    "    default_hyperparameters (dict): Default hyperparameters for the algorithms.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary of user-specified hyperparameters.\n",
    "    \"\"\"\n",
    "    hyperparameters = {}  # Dictionary to store user-specified hyperparameters\n",
    "    if algorithm_choice == 'all':\n",
    "        print(\"\\nDefault hyperparameters for all algorithms:\")\n",
    "        for algo, params in default_hyperparameters.items():\n",
    "            print(f\"{algo}: {params}\")  # Display default hyperparameters for each algorithm\n",
    "        use_defaults = input(\"Would you like to use these default hyperparameters? (yes/no) (default is yes): \").lower() == 'yes'\n",
    "        if not use_defaults:\n",
    "            for algo, params in default_hyperparameters.items():\n",
    "                print(f\"\\nEnter hyperparameters for {algo}:\")\n",
    "                hyperparameters[algo] = {}  # Initialize dictionary for each algorithm\n",
    "                for param, value in params.items():\n",
    "                    new_value = input(f\"{param} (default={value}): \")  # Prompt user for hyperparameter value\n",
    "                    hyperparameters[algo][param] = type(value)(new_value) if new_value else value  # Convert input type\n",
    "    else:\n",
    "        print(f\"\\nDefault hyperparameters for {algorithm_choice}: {default_hyperparameters[algorithm_choice]}\")\n",
    "        for param, value in default_hyperparameters[algorithm_choice].items():\n",
    "            new_value = input(f\"{param} (default={value}): \")  # Prompt user for hyperparameter value\n",
    "            hyperparameters[param] = type(value)(new_value) if new_value else value  # Convert input type\n",
    "    return hyperparameters  # Return the hyperparameters\n",
    "\n",
    "# Main function to detect and remove outliers from CSV files in a directory\n",
    "def detect_and_remove_outliers(directory, algorithm_choice, hyperparameters=None, use_window=True, window_size=15, step_size=5, drop_repeated=True, window_method='lowest_quantile', num_lowest_points=3):\n",
    "    \"\"\"\n",
    "    Detects and removes outliers from CSV files in the specified directory using PyCaret and/or a window method.\n",
    "    \n",
    "    Args:\n",
    "    directory (str): The directory path containing CSV files to process.\n",
    "    algorithm_choice (str): The anomaly detection algorithm(s) to use ('all' or specific algorithm).\n",
    "    hyperparameters (dict): Hyperparameters for the selected algorithm(s).\n",
    "    use_window (bool): Whether to use the rolling window method for initial outlier detection.\n",
    "    window_size (int): The size of the rolling window (default is 15).\n",
    "    step_size (int): The step size for the rolling window (default is 5).\n",
    "    drop_repeated (bool): Whether to drop files with repeated IDs.\n",
    "    window_method (str): The method for detecting window outliers ('lowest_quantile', 'both_quantiles', 'lowest_points').\n",
    "    num_lowest_points (int): Number of lowest points to remove if using 'lowest_points' method.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of results containing the original, cleaned data, and outlier information.\n",
    "    \"\"\"\n",
    "    counts = count_repeated_files(directory)  # Count repeated files by ID\n",
    "    repeated_ids = {key for key, count in counts.items() if count > 1}  # Extract repeated IDs\n",
    "    processed_keywords = ['lof', 'abod', 'cluster', 'knn', 'cof', 'iforest']  # Keywords indicating processed files\n",
    "    files = [f for f in os.listdir(directory) if 'clear' in f and f.endswith('.csv') and not any(keyword in f for keyword in processed_keywords)]  # Filter files to select files to apply noise removing\n",
    "    if drop_repeated:\n",
    "        files = [f for f in files if not any(repeated_id in f for repeated_id in repeated_ids)]  # Remove repeated files\n",
    "    results = []  # List to store results\n",
    "    \n",
    "    # Define default hyperparameters for each algorithm\n",
    "    default_hyperparameters = {\n",
    "        'knn': {'fraction': 0.1, 'n_neighbors': 20},\n",
    "        'lof': {'fraction': 0.1, 'n_neighbors': 20},\n",
    "        'abod': {'fraction': 0.1, 'n_neighbors': 3},\n",
    "        'cof': {'fraction': 0.1, 'n_neighbors': 3},\n",
    "        'cluster': {'fraction': 0.1, 'n_clusters': 3},\n",
    "        'iforest': {'fraction': 0.1, 'n_estimators': 200}\n",
    "    }\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)  # Full path of the file\n",
    "        data = pd.read_csv(file_path)  # Load data from CSV\n",
    "        if data.shape[1] < 2:\n",
    "            print(f\"Skipping {file} as it has less than two columns.\")  # Skip files with less than 2 columns\n",
    "            continue\n",
    "        target = data.columns[1]  # Assume the second column is the target for anomaly detection\n",
    "        print(f\"Processing file: {file}\")\n",
    "        print(f\"Data shape: {data.shape}\")\n",
    "        print(data.head())  # Display the first few rows of the data\n",
    "        \n",
    "        if use_window:\n",
    "            outlier_indices = window_outliers(data[target], window_size, step_size, window_method, num_lowest_points)  # Detect outliers using the window method\n",
    "            window_outliers_data = data.loc[outlier_indices]  # Data corresponding to window outliers\n",
    "            data = data.drop(index=outlier_indices)  # Remove outliers from the data\n",
    "            print(f\"{file} - Window: Removed {len(outlier_indices)} outliers\")  # Log the number of removed outliers\n",
    "        else:\n",
    "            window_outliers_data = pd.DataFrame(columns=data.columns)  # Initialize empty DataFrame if window method is not used\n",
    "        \n",
    "        if algorithm_choice == 'window':\n",
    "            cleaned_data = data  # Use data directly if only the window method is chosen\n",
    "            results.append((file, data, cleaned_data, window_outliers_data, data.columns[0], target, 'window', len(outlier_indices) / len(data) * 100))  # Add results\n",
    "        else:\n",
    "            exp = setup_pycaret(data)  # Setup PyCaret for anomaly detection\n",
    "            selected_algorithms = ['lof', 'abod', 'cluster', 'knn', 'cof', 'iforest'] if algorithm_choice == 'all' else [algorithm_choice]  # Select algorithms\n",
    "            all_outliers_indices = []  # List to store outlier indices for all algorithms\n",
    "            \n",
    "            for algo in selected_algorithms:\n",
    "                try:\n",
    "                    # Retrieve hyperparameters for the current algorithm, use defaults if not provided\n",
    "                    params = hyperparameters.get(algo, default_hyperparameters[algo]) if algorithm_choice == 'all' else hyperparameters\n",
    "                    \n",
    "                    # Create and assign model with specified hyperparameters\n",
    "                    model = create_model(algo, **params)\n",
    "                    result = assign_model(model)\n",
    "                    \n",
    "                    # Detect outliers\n",
    "                    outliers = result[result['Anomaly'] == 1]\n",
    "                    outlier_indices = outliers.index\n",
    "                    \n",
    "                    # Add detected outlier indices to the list\n",
    "                    all_outliers_indices.append(set(outlier_indices))\n",
    "                    print(f\"{file} - {algo}: Found {len(outlier_indices)} outliers\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file} with {algo}: {e}\")\n",
    "            \n",
    "            # Calculate common outliers if 'all' algorithms are used\n",
    "            if algorithm_choice == 'all':\n",
    "                if all_outliers_indices:\n",
    "                    common_outliers = set.intersection(*all_outliers_indices)\n",
    "                else:\n",
    "                    common_outliers = set()\n",
    "                outlier_percent = len(common_outliers) / len(data) * 100\n",
    "                cleaned_data = data.drop(index=common_outliers)\n",
    "                results.append((file, data, cleaned_data, window_outliers_data, data.columns[0], target, 'all', outlier_percent))\n",
    "            else:\n",
    "                for algo, outlier_indices in zip(selected_algorithms, all_outliers_indices):\n",
    "                    cleaned_data = data.drop(index=outlier_indices)\n",
    "                    results.append((file, data, cleaned_data, window_outliers_data, data.columns[0], target, algo, len(outlier_indices) / len(data) * 100))\n",
    "    \n",
    "    return results  # Return results\n",
    "\n",
    "# Function to plot the results of outlier detection and removal\n",
    "def plot_results(results):\n",
    "    \"\"\"\n",
    "    Plots the original, cleaned, and window-detected outliers for each processed file.\n",
    "    \n",
    "    Args:\n",
    "    results (list): List of tuples, each containing information about original data, cleaned data, and outliers.\n",
    "    \"\"\"\n",
    "    for file, data, cleaned_data, window_outliers_data, x_col, y_col, algo, outlier_percent in results:\n",
    "        plt.figure(figsize=(10, 5))  # Set figure size\n",
    "        plt.scatter(data[x_col], data[y_col], color='blue', label='Commonly Detected Outliers by All Algorithms')  # Plot original data\n",
    "        plt.scatter(window_outliers_data[x_col], window_outliers_data[y_col], color='green', label='Window Outliers')  # Plot window outliers\n",
    "        plt.scatter(cleaned_data[x_col], cleaned_data[y_col], color='red', label='Cleaned Data')  # Plot cleaned data\n",
    "        plt.xlabel(x_col)  # Set x-axis label\n",
    "        plt.ylabel(y_col)  # Set y-axis label\n",
    "        plt.yscale(\"log\")  # Set y-axis to logarithmic scale\n",
    "        plt.title(f'Outlier Removal using {algo}')  # Set plot title\n",
    "        plt.legend()  # Show legend\n",
    "        plt.show()  # Display plot\n",
    "        print(f\"File: {file} - {algo}: Removed {outlier_percent:.2f}% of data as outliers\")  # Log outlier removal info\n",
    "\n",
    "# Function to handle user choices for plotting and saving results\n",
    "def handle_user_choice(choice, results, directory):\n",
    "    \"\"\"\n",
    "    Handles user choice for plotting and/or saving cleaned data after outlier detection.\n",
    "    \n",
    "    Args:\n",
    "    choice (int): User's choice for action (1 = plot only, 2 = plot and save, 3 = save only).\n",
    "    results (list): List of tuples, each containing information about original data, cleaned data, and outliers.\n",
    "    directory (str): Path to the directory where cleaned files will be saved.\n",
    "    \"\"\"\n",
    "    if choice == 1:\n",
    "        plot_results(results)  # Plot results only\n",
    "    elif choice == 2:\n",
    "        plot_results(results)  # Plot and save results\n",
    "        for file, data, cleaned_data, window_outliers_data, x_col, y_col, algo, outlier_percent in results:\n",
    "            cleaned_file_name = f\"{os.path.splitext(file)[0]}_{algo}.csv\"  # Generate new filename for cleaned data\n",
    "            cleaned_file_path = os.path.join(directory, cleaned_file_name)  # Full path for the cleaned file\n",
    "            cleaned_data.to_csv(cleaned_file_path, index=False)  # Save cleaned data to CSV\n",
    "    elif choice == 3:\n",
    "        for file, data, cleaned_data, window_outliers_data, x_col, y_col, algo, outlier_percent in results:\n",
    "            cleaned_file_name = f\"{os.path.splitext(file)[0]}_{algo}.csv\"  # Generate new filename for cleaned data\n",
    "            cleaned_file_path = os.path.join(directory, cleaned_file_name)  # Full path for the cleaned file\n",
    "            cleaned_data.to_csv(cleaned_file_path, index=False)  # Save cleaned data to CSV\n",
    "    else:\n",
    "        print(\"Invalid choice. Please select 1, 2, or 3.\")  # Handle invalid user choice\n",
    "\n",
    "# Main execution starts here\n",
    "directory = input(\"Enter the directory path where the CSV files exist: \")  # Prompt user for directory path\n",
    "\n",
    "# Count repeated files in the specified directory\n",
    "counts = count_repeated_files(directory)\n",
    "total_files = len(os.listdir(directory))  # Total number of files in the directory\n",
    "total_repeated_files = sum(count for count in counts.values() if count > 1)  # Total number of repeated files\n",
    "total_non_repeated_files = total_files - total_repeated_files  # Total number of non-repeated files\n",
    "\n",
    "# Display counts of files\n",
    "print(f\"\\nTotal files in directory: {total_files}\")\n",
    "print(f\"Total repeated files: {total_repeated_files}\")\n",
    "print(f\"Total non-repeated files: {total_non_repeated_files}\")\n",
    "\n",
    "# Prompt user for choices regarding file processing\n",
    "drop_repeated_choice = int(input(\"\\nEnter 1 to drop repeated files or 2 to process all files (default is 1, i.e. drop repeated files): \") or 2)  # User choice for dropping repeated files\n",
    "use_window = input(\"Would you like to use Window outliers before any other algorithms? (yes/no) (default yes): \").lower() == 'yes'  # User choice for using window method\n",
    "\n",
    "# If using the window method, prompt for parameters\n",
    "if use_window:\n",
    "    window_size = int(input(\"Enter the window size (default is 15 points): \") or 15)  # Prompt for window size\n",
    "    step_size = int(input(\"Enter the step size (default is 5 point): \") or 5)  # Prompt for step size\n",
    "\n",
    "    print(\"Choose the method for window outliers detection:\")\n",
    "    print(\"1. Drop the lowest quantile in the window\")\n",
    "    print(\"2. Drop both the highest and lowest quantiles in the window\")\n",
    "    print(\"3. Drop the lowest point(s) in the window\")\n",
    "    window_method_choice = int(input(\"Enter 1 to drop only the lowest quantile in the window, 2 to drop both the lowest and highest quantiles in the window, or 3 to drop the lowest point/s in the window (default is 3): \") or 3)  # User choice for window method\n",
    "\n",
    "    if window_method_choice == 1:\n",
    "        window_method = 'lowest_quantile'\n",
    "        num_lowest_points = 1  # Default value, not used in this method\n",
    "    elif window_method_choice == 2:\n",
    "        window_method = 'both_quantiles'\n",
    "        num_lowest_points = 1  # Default value, not used in this method\n",
    "    elif window_method_choice == 3:\n",
    "        window_method = 'lowest_points'\n",
    "        num_lowest_points = int(input(\"Enter the number of lowest points to remove in each window (default is 3): \") or 3)  # Prompt for number of lowest points\n",
    "    else:\n",
    "        raise ValueError(\"Invalid choice for window outliers detection method\")  # Handle invalid method choice\n",
    "else:\n",
    "    window_size = 15\n",
    "    step_size = 5\n",
    "    window_method = 'lowest_quantile'  # Default value, not used if use_window is False\n",
    "    num_lowest_points = 3  # Default value, not used if use_window is False\n",
    "\n",
    "# Prompt user for algorithm choice\n",
    "algorithm_choice = input(\"Choose the algorithm(s) from ['knn', 'lof', 'abod', 'cof', 'cluster', 'iforest'] or type 'all'): \").lower()\n",
    "\n",
    "# Define default hyperparameters for each algorithm\n",
    "default_hyperparameters = {\n",
    "    'knn': {'fraction': 0.10, 'n_neighbors': 20},\n",
    "    'lof': {'fraction': 0.10, 'n_neighbors': 20},\n",
    "    'abod': {'fraction': 0.10, 'n_neighbors': 3},\n",
    "    'cof': {'fraction': 0.10, 'n_neighbors': 3},\n",
    "    'cluster': {'fraction': 0.10, 'n_clusters': 3},\n",
    "    'iforest': {'fraction': 0.10, 'n_estimators': 200}\n",
    "}\n",
    "\n",
    "# Get hyperparameters based on user input\n",
    "hyperparameters = get_hyperparameters(algorithm_choice, default_hyperparameters)\n",
    "\n",
    "# Prompt user for choice of plotting and saving results\n",
    "user_choice = int(input(\"Enter 1 for plot only, 2 for plot and save, or 3 for save only (default is 1, i.e. show plots only): \") or 1)\n",
    "\n",
    "# Determine whether to drop repeated files\n",
    "drop_repeated = (drop_repeated_choice == 1)\n",
    "\n",
    "# Detect and remove outliers from the specified directory\n",
    "results = detect_and_remove_outliers(directory, algorithm_choice, hyperparameters, use_window, window_size, step_size, drop_repeated, window_method, num_lowest_points)\n",
    "\n",
    "# Handle user choice for plotting and saving results\n",
    "handle_user_choice(user_choice, results, directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
